{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mCBF_2Dto3D_PoseLifting",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHTs0iziFtoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d85031-da8b-4b94-f56e-7ab80641ba65"
      },
      "source": [
        "#Notebook intended for a Google Colab Session\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IoBDsHjF3Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dc850f-bb7c-4a4b-e78d-0c8503c50b8a"
      },
      "source": [
        "cd gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YteREdQiGmE0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6Iy-aOJuxR"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "num_epochs = 5000000\n",
        "lin_s = 1024\n",
        "nstage = 2\n",
        "pdrop = 0.67\n",
        "loss_rate = 0.0001\n",
        "\n",
        "train_2d_x_path = './mCBF/train/mouse-wheel_revisions_synthetic-v2_camera-2_calibrated_2d.csv' # path to 2D pixel coords saved by Blender\n",
        "train_3d_y_path = './mCBF/train/mouse-wheel_revisions_synthetic-v2_camera-2_calibrated_3d.csv' # path to 3D realtive coords saved by Blender\n",
        "model_save_path = './mCBF/checkpoints/' # path to models\n",
        "model_name = 'mouse-wheel_camera2_model' # name of model to save/load\n",
        "test_path = './mCBF/camera2_2d/' # path to exported .CSVs from DeepLabCut to lift to 3D\n",
        "save_path = './mCBF/camera2_lifted3d/' # path to save lifted 3d coords\n",
        "best_loss = 0.00008 # Model will begin saving once loss is below this level and saved each subsequent better model. We halted training at around a loss of 5e-5 to 8e-5 (Interrupt execution when you want to halt)\n",
        "noise_amount = 2 # Amount of noise to add to pixel coordinates +=noise_amount/2 for better generalizability "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kXam4szJk7o"
      },
      "source": [
        "## MODEL BASED ON Martinez et al. 2017 https://github.com/una-dinosauria/3d-pose-baseline\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal(m.weight)\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, linear_size, p_dropout=0.5):\n",
        "        super(Linear, self).__init__()\n",
        "        self.l_size = linear_size\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.w1 = nn.Linear(self.l_size, self.l_size)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(self.l_size)\n",
        "\n",
        "        self.w2 = nn.Linear(self.l_size, self.l_size)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(self.l_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.w1(x)\n",
        "        y = self.batch_norm1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.dropout(y)\n",
        "\n",
        "        y = self.w2(y)\n",
        "        y = self.batch_norm2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.dropout(y)\n",
        "\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 linear_size=1024,\n",
        "                 num_stage=2,\n",
        "                 p_dropout=0.5):\n",
        "        super(LinearModel, self).__init__()\n",
        "\n",
        "        self.linear_size = linear_size\n",
        "        self.p_dropout = p_dropout\n",
        "        self.num_stage = num_stage\n",
        "\n",
        "        # 2d joints\n",
        "        self.input_size =  num_bones * 2\n",
        "        # 3d joints\n",
        "        self.output_size = num_bones * 3\n",
        "\n",
        "        # process input to linear size\n",
        "        self.w1 = nn.Linear(self.input_size, self.linear_size)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(self.linear_size)\n",
        "\n",
        "        self.linear_stages = []\n",
        "        for l in range(num_stage):\n",
        "            self.linear_stages.append(Linear(self.linear_size, self.p_dropout))\n",
        "        self.linear_stages = nn.ModuleList(self.linear_stages)\n",
        "\n",
        "        # post processing\n",
        "        self.w2 = nn.Linear(self.linear_size, self.output_size)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(self.p_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pre-processing\n",
        "        y = self.w1(x)\n",
        "        y = self.batch_norm1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.dropout(y)\n",
        "\n",
        "        # linear layers\n",
        "        for i in range(self.num_stage):\n",
        "            y = self.linear_stages[i](y)\n",
        "\n",
        "        y = self.w2(y)\n",
        "\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znO9Tt_YGn23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c6ca06-1086-4b8e-982a-5bc2117f4265"
      },
      "source": [
        "train_x = np.genfromtxt(train_2d_x_path,delimiter=',',dtype=str)\n",
        "train_y = np.genfromtxt(train_3d_y_path,delimiter=',',dtype=str)\n",
        "\n",
        "#Initialize numpy arrays with input variables and expected output currently it is pixel coordinates of markers becoming mCBF coordinates\n",
        "num_frames = train_x.shape[0] - 3\n",
        "num_bones = int((train_x.shape[1] - 1)/2)\n",
        "\n",
        "print(\"Training with \" + str(num_frames) + \" frames and \" + str(num_bones) + \" bones\")\n",
        "\n",
        "x_array = np.zeros((num_frames,num_bones*2),dtype=\"float32\")\n",
        "noise_array = noise_amount * np.random.random_sample((num_frames,num_bones*2)) - (noise_amount/2)\n",
        "\n",
        "for i in range(num_frames):\n",
        "    x_array[i] = train_x[i+3][1:]\n",
        "\n",
        "x_array = np.add(x_array,noise_array) #Add a bit of noise to make it more generalizable\n",
        "\n",
        "y_array = np.zeros((num_frames,num_bones*3),dtype=float)\n",
        "for i in range(num_frames):\n",
        "    y_array[i] = train_y[i+3][1:]\n",
        "    \n",
        "X = torch.from_numpy(x_array).float().to(device)\n",
        "Y = torch.from_numpy(y_array).float().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with 1000 frames and 28 bones\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlWpJ6nvKKaq"
      },
      "source": [
        "model = LinearModel(linear_size=lin_s,num_stage=nstage,p_dropout=pdrop)\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=loss_rate)\n",
        "epoch = 0\n",
        "\n",
        "checkpoint = {\n",
        "    \"epoch\" : epoch,\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"optim_state\": optimizer.state_dict()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9QU91oQKrg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71618103-af1f-4095-b7f6-c6584b22e429"
      },
      "source": [
        "for t in range(epoch, num_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    epoch = t\n",
        "    y_pred = model(X)\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, Y)\n",
        "\n",
        "    if t % 300 == 299:\n",
        "        print(t, loss.item())\n",
        "        if(loss.item() < best_loss):\n",
        "            best_loss = loss.item()\n",
        "            path =  model_save_path + model_name +'.pth'\n",
        "            torch.save(checkpoint, path)\n",
        "            print(\"Saved checkpoint!\")\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "299 0.19476483762264252\n",
            "599 0.06873439997434616\n",
            "899 0.027795903384685516\n",
            "1199 0.012249166145920753\n",
            "1499 0.005494280252605677\n",
            "1799 0.0026363444048911333\n",
            "2099 0.0013426363002508879\n",
            "2399 0.0007987627177499235\n",
            "2699 0.0005343255470506847\n",
            "2999 0.0004119911463931203\n",
            "3299 0.0003630859137047082\n",
            "3599 0.0003217408957425505\n",
            "3899 0.0003168782277498394\n",
            "4199 0.00029598455876111984\n",
            "4499 0.00027291892911307514\n",
            "4799 0.0002637551515363157\n",
            "5099 0.00025639100931584835\n",
            "5399 0.0002502524002920836\n",
            "5699 0.00024685278185643256\n",
            "5999 0.00022546196123585105\n",
            "6299 0.00022497435566037893\n",
            "6599 0.00022146910487208515\n",
            "6899 0.0002160643634852022\n",
            "7199 0.00021489635400939733\n",
            "7499 0.00020762631902471185\n",
            "7799 0.0002002070687012747\n",
            "8099 0.00019328604685142636\n",
            "8399 0.00018308908329345286\n",
            "8699 0.0001700902939774096\n",
            "8999 0.0001742908643791452\n",
            "9299 0.000160323572345078\n",
            "9599 0.0001665660965954885\n",
            "9899 0.00015771920152474195\n",
            "10199 0.00015271453594323248\n",
            "10499 0.00015887862537056208\n",
            "10799 0.00014195943367667496\n",
            "11099 0.0001456620084354654\n",
            "11399 0.0001320766023127362\n",
            "11699 0.00013353321992326528\n",
            "11999 0.0001336212590103969\n",
            "12299 0.00012329242599662393\n",
            "12599 0.0001227155007654801\n",
            "12899 0.00011789691779995337\n",
            "13199 0.00012039281864417717\n",
            "13499 0.00011115388770122081\n",
            "13799 0.00010929954441962764\n",
            "14099 0.00010786599887069315\n",
            "14399 0.00010765351180452853\n",
            "14699 0.00010475528688402846\n",
            "14999 9.91184133454226e-05\n",
            "15299 0.00010377097351010889\n",
            "15599 0.0001035584500641562\n",
            "15899 9.694619075162336e-05\n",
            "16199 9.249888535123318e-05\n",
            "16499 8.974210504675284e-05\n",
            "16799 8.573562081437558e-05\n",
            "17099 9.755594510352239e-05\n",
            "17399 8.267252997029573e-05\n",
            "17699 8.70584772201255e-05\n",
            "17999 8.078872633632272e-05\n",
            "18299 8.209786028601229e-05\n",
            "18599 7.477030158042908e-05\n",
            "Saved checkpoint!\n",
            "18899 7.186635048128664e-05\n",
            "Saved checkpoint!\n",
            "19199 7.70546612329781e-05\n",
            "19499 7.474124868167564e-05\n",
            "19799 7.008319516899064e-05\n",
            "Saved checkpoint!\n",
            "20099 7.371339597739279e-05\n",
            "20399 7.734251266811043e-05\n",
            "20699 7.957936031743884e-05\n",
            "20999 7.016768358880654e-05\n",
            "21299 7.9112927778624e-05\n",
            "21599 6.725905404891819e-05\n",
            "Saved checkpoint!\n",
            "21899 6.601373024750501e-05\n",
            "Saved checkpoint!\n",
            "22199 6.329623283818364e-05\n",
            "Saved checkpoint!\n",
            "22499 6.337997183436528e-05\n",
            "22799 6.769323226762936e-05\n",
            "23099 6.96045535732992e-05\n",
            "23399 6.444266909966245e-05\n",
            "23699 6.65943807689473e-05\n",
            "23999 7.404480857076123e-05\n",
            "24299 5.9178601077292114e-05\n",
            "Saved checkpoint!\n",
            "24599 6.731536268489435e-05\n",
            "24899 6.0302023484837264e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-29da8a9a7d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Zero gradients, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He3f0suxSqXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ba659e-fe65-4473-ae88-c64348fb57e5"
      },
      "source": [
        "camera_paths = []\n",
        "camera_filenames = []\n",
        "\n",
        "for dirName, subdirList, fileList in os.walk(test_path):\n",
        "    fileList.sort()\n",
        "    for fname in fileList:\n",
        "        camera_paths.append(dirName+fname)\n",
        "        camera_filenames.append(fname[:-4])\n",
        "       \n",
        "print(\"There are {} files for the single camera\".format(len(camera_filenames)))\n",
        "print(camera_filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 files for the single camera\n",
            "['2020_12_2_MK1_1606954279_camera-1DLC_resnet50_camera_2_3Dec11shuffle1_1030000filtered', '2020_12_2_MK1_1606954279_camera-1DLC_resnet50_camera_2_3Dec11shuffle1_600000filtered']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYu32xYCTsUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58b86d2-4051-448b-8b9f-28002f84ce8a"
      },
      "source": [
        "model_path = model_save_path + model_name + '.pth'\n",
        "epoch = 0\n",
        "loaded = torch.load(model_path, map_location={'cuda:0': 'cpu'})\n",
        "epoch = loaded[\"epoch\"]\n",
        "\n",
        "model = LinearModel(linear_size=lin_s,num_stage=nstage,p_dropout=pdrop)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=loss_rate)\n",
        "\n",
        "model.load_state_dict(loaded[\"model_state\"])\n",
        "optimizer.load_state_dict(loaded[\"optim_state\"])\n",
        "\n",
        "for i in range(len(camera_filenames)):\n",
        "    print(\"Lifting file number: \", (i+1))\n",
        "    filename = camera_filenames[i]\n",
        "    filepath = camera_paths[i]\n",
        "    \n",
        "    #LOAD DATA TO TORCH\n",
        "    dlc_in_cam1 = np.genfromtxt(filepath,delimiter=',',dtype=str)\n",
        "\n",
        "    joints = np.zeros(int((dlc_in_cam1.shape[1]-1)/3), dtype=object)\n",
        "    test_rows = dlc_in_cam1.shape[0]-3\n",
        "    test_cols = len(joints) * 2\n",
        "\n",
        "    x_test = np.zeros((test_rows,test_cols),dtype=float)\n",
        "    if(test_rows > 1):\n",
        "        for c in range(len(joints)):\n",
        "            joints[c] = dlc_in_cam1[1][c*3+1]\n",
        "        for r in range(test_rows):\n",
        "            for c in range(num_bones):\n",
        "                x_test[r][c*2] = dlc_in_cam1[r+3][c*3+1]\n",
        "                x_test[r][c*2+1] = dlc_in_cam1[r+3][c*3+2]\n",
        "  \n",
        "\n",
        "        X_test = torch.from_numpy(x_test).float().to(device)\n",
        "\n",
        "        #Compute test\n",
        "        model.to(device)\n",
        "        y_pred_test = model(X_test)\n",
        "        #print(y_pred_test)\n",
        "\n",
        "        dlc_out = np.zeros((y_pred_test.shape[0]+3, num_bones*3+1), dtype=object)\n",
        "        dlc_out[0][0] = \"scorer\"\n",
        "        dlc_out[1][0] = \"bodypart\"\n",
        "        dlc_out[2][0] = \"relatice coordinates\"\n",
        "        for c in range(num_bones):\n",
        "            dlc_out[0][c*3+1] = \"pose-estimator\"\n",
        "            dlc_out[0][c*3+2] = \"pose-estimator\"\n",
        "            dlc_out[0][c*3+3] = \"pose-estimator\"\n",
        "            dlc_out[1][c*3+1] = joints[c]\n",
        "            dlc_out[1][c*3+2] = joints[c]\n",
        "            dlc_out[1][c*3+3] = joints[c]\n",
        "            dlc_out[2][c*3+1] = \"x\"\n",
        "            dlc_out[2][c*3+2] = \"y\"\n",
        "            dlc_out[2][c*3+3] = \"z\"\n",
        "\n",
        "        for r in range(y_pred_test.shape[0]):\n",
        "            dlc_out[r+3][0] = \"frame_\"+str(r)\n",
        "            dlc_out[r+3][1:] = y_pred_test.cpu().detach().numpy()[r]\n",
        "        #print(dlc_out)\n",
        "\n",
        "        outputfile = save_path + filename + \"_LIFTED-3d.csv\"\n",
        "\n",
        "        import csv\n",
        "\n",
        "        with open(outputfile, mode='w',newline='') as f:\n",
        "            w = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "            w.writerows(dlc_out)\n",
        "    else:\n",
        "        print(\"single frame video ignoring...\" + file)\n",
        "    \n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lifting file number:  1\n",
            "Lifting file number:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_EKtccWi6du"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}